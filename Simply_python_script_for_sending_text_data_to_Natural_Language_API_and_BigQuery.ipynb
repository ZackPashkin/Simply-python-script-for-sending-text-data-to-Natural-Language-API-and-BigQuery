{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simply python script for sending text data to Natural Language API and BigQuery.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ZackPashkin/Simply-python-script-for-sending-text-data-to-Natural-Language-API-and-BigQuery/blob/master/Simply_python_script_for_sending_text_data_to_Natural_Language_API_and_BigQuery.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gsG6h9ZVuMma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What is it good for?**\n",
        "\n",
        "*    sending the text data to the Natural Language API\n",
        "*    classifying articles\n",
        "*    importing articles to BigQuery\n",
        "\n",
        "\n",
        "Create a file called *classify-text.py* and copy the following into it. \n",
        "Replace YOUR_PROJECT with your GCP Project ID."
      ]
    },
    {
      "metadata": {
        "id": "h8XtVCwWtX7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.cloud import storage, language, bigquery\n",
        "\n",
        "# Set up our GCS, NL, and BigQuery clients\n",
        "storage_client = storage.Client()\n",
        "nl_client = language.LanguageServiceClient()\n",
        "# just replace YOUR_PROJECT with your project id below \n",
        "# it should look like this: bq_client = bigquery.Client(project='gcp-f1d55e5e42f12d87')\n",
        "bq_client = bigquery.Client(project='YOUR_PROJECT')\n",
        "\n",
        "dataset_ref = bq_client.dataset('news_classification_dataset')\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "table_ref = dataset.table('article_data') # Update this if you used a different table name\n",
        "table = bq_client.get_table(table_ref)\n",
        "\n",
        "# Send article text to the NL API's classifyText method\n",
        "def classify_text(article):\n",
        "        response = nl_client.classify_text(\n",
        "                document=language.types.Document(\n",
        "                        content=article,\n",
        "                        type=language.enums.Document.Type.PLAIN_TEXT\n",
        "                )\n",
        "        )\n",
        "        return response\n",
        "\n",
        "rows_for_bq = []\n",
        "files = storage_client.bucket('text-classification-codelab').list_blobs()\n",
        "print(\"Got article files from GCS, sending them to the NL API (this will take ~2 minutes)...\")\n",
        "\n",
        "# Send files to the NL API and save the result to send to BigQuery\n",
        "for file in files:\n",
        "        if file.name.endswith('txt'):\n",
        "                article_text = file.download_as_string()\n",
        "                nl_response = classify_text(article_text)\n",
        "                if len(nl_response.categories) > 0:\n",
        "                        rows_for_bq.append((article_text, nl_response.categories[0].name, nl_response.categories[0].confidence))\n",
        "\n",
        "print(\"Writing NL API article data to BigQuery...\")\n",
        "# Write article text + category data to BQ\n",
        "errors = bq_client.insert_rows(table, rows_for_bq)\n",
        "assert errors == []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jio2h99cupIG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " And run the following script:\n",
        " \n",
        "** python classify-text.py**\n"
      ]
    },
    {
      "metadata": {
        "id": "mNjZ2T7Xvxnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What script is actually doing:\n",
        "\n",
        "We're using the google-cloud Python client library to access Cloud Storage, the Natural Language API, and BigQuery.\n",
        " .  When classifying each article is done, the data is inserted into BigQuery using insert_rows().\n",
        "\n",
        "*   First, a client is created for each service.\n",
        "*   Then references are created to the BigQuery table.\n",
        "*     *files* is a reference to each of the dataset files in the public bucket.\n",
        "*   We iterate through these files, download the articles as strings, and send each one to the Natural Language API in our classify_text function.\n",
        "*   For all articles where the Natural Language API returns a category, the article and its category data are saved to a rows_for_bq list.\n",
        "*   When classifying each article is done, the data is inserted into BigQuery using insert_rows().\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IC3WDoOSxshn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "*The Natural Language API can return more than one category for a document, but for this script you're only storing the first category returned, to keep things simple.*"
      ]
    },
    {
      "metadata": {
        "id": "OKcdENe7xJLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Enjoy the script!**"
      ]
    }
  ]
}